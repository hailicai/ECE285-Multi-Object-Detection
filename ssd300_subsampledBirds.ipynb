{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c1yeung\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py:29: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\c1yeung\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.CSRRD7HKRKC3T3YXA7VY7TAZGLSWDKW6.gfortran-win_amd64.dll\n",
      "C:\\Users\\c1yeung\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from scipy.misc import imread\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from eval_utils.average_precision_evaluator import Evaluator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a few configuration parameters.\n",
    "img_height = 300\n",
    "img_width = 300\n",
    "n_classes = 200\n",
    "model_mode = 'inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\c1yeung\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\c1yeung\\Downloads\\ssd_keras-master\\ssd_keras-master\\keras_layers\\keras_layer_DecodeDetections.py:174: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\c1yeung\\Downloads\\ssd_keras-master\\ssd_keras-master\\keras_loss_function\\keras_ssd_loss.py:166: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# # 1: Build the Keras model\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, 3),\n",
    "                n_classes=n_classes,\n",
    "                mode=model_mode,\n",
    "                l2_regularization=0.0005,\n",
    "                scales=[0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05], # The scales for MS COCO [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]\n",
    "                aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5]],\n",
    "                two_boxes_for_ar1=True,\n",
    "                steps=[8, 16, 32, 64, 100, 300],\n",
    "                offsets=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "                clip_boxes=False,\n",
    "                variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                normalize_coords=True,\n",
    "                subtract_mean=[123, 117, 104],\n",
    "                swap_channels=[2, 1, 0],\n",
    "                confidence_thresh=0.01,\n",
    "                iou_threshold=0.45,\n",
    "                top_k=200,\n",
    "                nms_max_output_size=400)\n",
    "\n",
    "# 2: Load the trained weights into the model.\n",
    "\n",
    "# TODO: Set the path of the trained weights.\n",
    "# weights_path = 'VGG_VOC0712_SSD_300x300_iter_120000.h5'\n",
    "weights_path = 'VGG_coco_SSD_300x300_iter_400000_subsampled_bird_classes.h5'\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 3: Compile the model so that Keras won't complain the next time you load it.\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'withbirds1.h5'\n",
    "\n",
    "# # We need to create an SSDLoss object in order to pass that to the model loader.\n",
    "# ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "# K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "# model = load_model(model_path, custom_objects={'AnchorBoxes': AnchorBoxes,\n",
    "#                                                'L2Normalization': L2Normalization,\n",
    "#                                                'DecodeDetections': DecodeDetections,\n",
    "#                                                'compute_loss': ssd_loss.compute_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./birds/classes.txt','r') as f:\n",
    "    content = f.readlines()\n",
    "classes = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n"
     ]
    }
   ],
   "source": [
    "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "# TODO: Set the paths to the dataset here.\n",
    "Pascal_VOC_dataset_images_dir = './VOC2007/VOC2007/JPEGImages/'\n",
    "Pascal_VOC_dataset_annotations_dir = './VOC2007/VOC2007/Annotations/'\n",
    "Pascal_VOC_dataset_image_set_filename = './VOC2007/VOC2007/ImageSets/Main/test.txt'\n",
    "\n",
    "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "# classes = ['background','person','bird','cat','cow','dog','horse','sheep','aeroplane','bicycle',\n",
    "#           'boat','bus','car','motorbike','train','bottle','chair','diningtable','pottedplant','sofa','tvmonitor']\n",
    "\n",
    "\n",
    "# dataset.parse_xml(images_dirs=[Pascal_VOC_dataset_images_dir],\n",
    "#                   image_set_filenames=[Pascal_VOC_dataset_image_set_filename],\n",
    "#                   annotations_dirs=[Pascal_VOC_dataset_annotations_dir],\n",
    "#                   classes=classes,\n",
    "#                   include_classes='all',\n",
    "#                   exclude_truncated=False,\n",
    "#                   exclude_difficult=False,\n",
    "#                   ret=False)\n",
    "\n",
    "val_dataset.parse_csv(images_dir='./birds/birdimages/',\n",
    "                    labels_filename='./birds/val.csv',\n",
    "                    input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'],\n",
    "                    include_classes='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the evaluation dataset: 372\n",
      "\n",
      "Producing predictions batch-wise: 100%|████████████████████████████████████████████████| 12/12 [06:49<00:00, 31.38s/it]\n",
      "No predictions for class 1/200\n",
      "No predictions for class 2/200\n",
      "No predictions for class 3/200\n",
      "Matching predictions to ground truth, class 4/200.: 100%|███████████████████████| 1054/1054 [00:00<00:00, 75286.04it/s]\n",
      "Matching predictions to ground truth, class 5/200.: 100%|█████████████████████████| 407/407 [00:00<00:00, 58145.09it/s]\n",
      "No predictions for class 6/200\n",
      "No predictions for class 7/200\n",
      "Matching predictions to ground truth, class 8/200.: 100%|█████████████████████████| 888/888 [00:00<00:00, 59201.47it/s]\n",
      "No predictions for class 9/200\n",
      "No predictions for class 10/200\n",
      "No predictions for class 11/200\n",
      "Matching predictions to ground truth, class 12/200.: 100%|████████████████████████| 120/120 [00:00<00:00, 40006.08it/s]\n",
      "Matching predictions to ground truth, class 13/200.: 100%|██████████████████████| 4094/4094 [00:00<00:00, 71827.06it/s]\n",
      "No predictions for class 14/200\n",
      "No predictions for class 15/200\n",
      "Matching predictions to ground truth, class 16/200.: 100%|███████████████████████████| 12/12 [00:00<00:00, 6004.73it/s]\n",
      "No predictions for class 17/200\n",
      "Matching predictions to ground truth, class 18/200.: 100%|██████████████████████| 2218/2218 [00:00<00:00, 73936.93it/s]\n",
      "Matching predictions to ground truth, class 19/200.: 100%|██████████████████████| 4246/4246 [00:00<00:00, 81205.87it/s]\n",
      "No predictions for class 20/200\n",
      "No predictions for class 21/200\n",
      "No predictions for class 22/200\n",
      "No predictions for class 23/200\n",
      "No predictions for class 24/200\n",
      "No predictions for class 25/200\n",
      "No predictions for class 26/200\n",
      "Matching predictions to ground truth, class 27/200.: 100%|██████████████████████████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "Matching predictions to ground truth, class 28/200.: 100%|██████████████████████| 4126/4126 [00:00<00:00, 78257.46it/s]\n",
      "Matching predictions to ground truth, class 29/200.: 100%|██████████████████████| 2640/2640 [00:00<00:00, 81221.16it/s]\n",
      "No predictions for class 30/200\n",
      "No predictions for class 31/200\n",
      "Matching predictions to ground truth, class 32/200.: 100%|████████████████████████| 249/249 [00:00<00:00, 49801.23it/s]\n",
      "No predictions for class 33/200\n",
      "Matching predictions to ground truth, class 34/200.: 100%|██████████████████████████| 75/75 [00:00<00:00, 37493.78it/s]\n",
      "Matching predictions to ground truth, class 35/200.: 100%|██████████████████████████| 89/89 [00:00<00:00, 44503.23it/s]\n",
      "No predictions for class 36/200\n",
      "No predictions for class 37/200\n",
      "No predictions for class 38/200\n",
      "No predictions for class 39/200\n",
      "No predictions for class 40/200\n",
      "No predictions for class 41/200\n",
      "Matching predictions to ground truth, class 42/200.: 100%|█████████████████████████████| 2/2 [00:00<00:00, 2000.14it/s]\n",
      "Matching predictions to ground truth, class 43/200.: 100%|██████████████████████| 4084/4084 [00:00<00:00, 77059.07it/s]\n",
      "No predictions for class 44/200\n",
      "No predictions for class 45/200\n",
      "Matching predictions to ground truth, class 46/200.: 100%|█████████████████████████████| 4/4 [00:00<00:00, 3998.38it/s]\n",
      "Matching predictions to ground truth, class 47/200.: 100%|███████████████████████████| 20/20 [00:00<00:00, 9998.34it/s]\n",
      "No predictions for class 48/200\n",
      "No predictions for class 49/200\n",
      "No predictions for class 50/200\n",
      "Matching predictions to ground truth, class 51/200.: 100%|██████████████████████████| 32/32 [00:00<00:00, 32002.32it/s]\n",
      "No predictions for class 52/200\n",
      "No predictions for class 53/200\n",
      "No predictions for class 54/200\n",
      "No predictions for class 55/200\n",
      "No predictions for class 56/200\n",
      "No predictions for class 57/200\n",
      "No predictions for class 58/200\n",
      "No predictions for class 59/200\n",
      "No predictions for class 60/200\n",
      "Matching predictions to ground truth, class 61/200.: 100%|██████████████████████████| 12/12 [00:00<00:00, 12006.60it/s]\n",
      "No predictions for class 62/200\n",
      "No predictions for class 63/200\n",
      "Matching predictions to ground truth, class 64/200.: 100%|██████████████████████████| 80/80 [00:00<00:00, 40002.90it/s]\n",
      "No predictions for class 65/200\n",
      "No predictions for class 66/200\n",
      "No predictions for class 67/200\n",
      "Matching predictions to ground truth, class 68/200.: 100%|█████████████████████████████| 2/2 [00:00<00:00, 2000.62it/s]\n",
      "No predictions for class 69/200\n",
      "Matching predictions to ground truth, class 70/200.: 100%|███████████████████████████| 13/13 [00:00<00:00, 6496.60it/s]\n",
      "No predictions for class 71/200\n",
      "No predictions for class 72/200\n",
      "No predictions for class 73/200\n",
      "No predictions for class 74/200\n",
      "No predictions for class 75/200\n",
      "No predictions for class 76/200\n",
      "No predictions for class 77/200\n",
      "No predictions for class 78/200\n",
      "No predictions for class 79/200\n",
      "No predictions for class 80/200\n",
      "No predictions for class 81/200\n",
      "No predictions for class 82/200\n",
      "No predictions for class 83/200\n",
      "No predictions for class 84/200\n",
      "No predictions for class 85/200\n",
      "No predictions for class 86/200\n",
      "No predictions for class 87/200\n",
      "No predictions for class 88/200\n",
      "No predictions for class 89/200\n",
      "No predictions for class 90/200\n",
      "No predictions for class 91/200\n",
      "No predictions for class 92/200\n",
      "No predictions for class 93/200\n",
      "No predictions for class 94/200\n",
      "No predictions for class 95/200\n",
      "No predictions for class 96/200\n",
      "No predictions for class 97/200\n",
      "No predictions for class 98/200\n",
      "No predictions for class 99/200\n",
      "No predictions for class 100/200\n",
      "No predictions for class 101/200\n",
      "No predictions for class 102/200\n",
      "No predictions for class 103/200\n",
      "No predictions for class 104/200\n",
      "No predictions for class 105/200\n",
      "No predictions for class 106/200\n",
      "No predictions for class 107/200\n",
      "No predictions for class 108/200\n",
      "No predictions for class 109/200\n",
      "No predictions for class 110/200\n",
      "No predictions for class 111/200\n",
      "No predictions for class 112/200\n",
      "No predictions for class 113/200\n",
      "No predictions for class 114/200\n",
      "No predictions for class 115/200\n",
      "No predictions for class 116/200\n",
      "No predictions for class 117/200\n",
      "No predictions for class 118/200\n",
      "No predictions for class 119/200\n",
      "No predictions for class 120/200\n",
      "No predictions for class 121/200\n",
      "No predictions for class 122/200\n",
      "No predictions for class 123/200\n",
      "No predictions for class 124/200\n",
      "No predictions for class 125/200\n",
      "No predictions for class 126/200\n",
      "No predictions for class 127/200\n",
      "No predictions for class 128/200\n",
      "No predictions for class 129/200\n",
      "No predictions for class 130/200\n",
      "No predictions for class 131/200\n",
      "No predictions for class 132/200\n",
      "No predictions for class 133/200\n",
      "No predictions for class 134/200\n",
      "No predictions for class 135/200\n",
      "No predictions for class 136/200\n",
      "No predictions for class 137/200\n",
      "No predictions for class 138/200\n",
      "No predictions for class 139/200\n",
      "No predictions for class 140/200\n",
      "No predictions for class 141/200\n",
      "No predictions for class 142/200\n",
      "No predictions for class 143/200\n",
      "No predictions for class 144/200\n",
      "No predictions for class 145/200\n",
      "No predictions for class 146/200\n",
      "No predictions for class 147/200\n",
      "No predictions for class 148/200\n",
      "No predictions for class 149/200\n",
      "No predictions for class 150/200\n",
      "No predictions for class 151/200\n",
      "No predictions for class 152/200\n",
      "No predictions for class 153/200\n",
      "No predictions for class 154/200\n",
      "No predictions for class 155/200\n",
      "No predictions for class 156/200\n",
      "No predictions for class 157/200\n",
      "No predictions for class 158/200\n",
      "No predictions for class 159/200\n",
      "Matching predictions to ground truth, class 160/200.: 100%|███████████████████████| 290/290 [00:00<00:00, 72496.61it/s]\n",
      "No predictions for class 161/200\n",
      "No predictions for class 162/200\n",
      "Matching predictions to ground truth, class 163/200.: 100%|█████████████████████| 2407/2407 [00:00<00:00, 80239.79it/s]\n",
      "No predictions for class 164/200\n",
      "No predictions for class 165/200\n",
      "No predictions for class 166/200\n",
      "No predictions for class 167/200\n",
      "No predictions for class 168/200\n",
      "No predictions for class 169/200\n",
      "No predictions for class 170/200\n",
      "No predictions for class 171/200\n",
      "No predictions for class 172/200\n",
      "No predictions for class 173/200\n",
      "No predictions for class 174/200\n",
      "No predictions for class 175/200\n",
      "No predictions for class 176/200\n",
      "No predictions for class 177/200\n",
      "No predictions for class 178/200\n",
      "No predictions for class 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No predictions for class 180/200\n",
      "No predictions for class 181/200\n",
      "Matching predictions to ground truth, class 182/200.: 100%|███████████████████████| 513/513 [00:00<00:00, 64127.74it/s]\n",
      "Matching predictions to ground truth, class 183/200.: 100%|█████████████████████| 1971/1971 [00:00<00:00, 75809.02it/s]\n",
      "No predictions for class 184/200\n",
      "Matching predictions to ground truth, class 185/200.: 100%|████████████████████████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Matching predictions to ground truth, class 186/200.: 100%|███████████████████████| 188/188 [00:00<00:00, 62671.21it/s]\n",
      "No predictions for class 187/200\n",
      "No predictions for class 188/200\n",
      "No predictions for class 189/200\n",
      "No predictions for class 190/200\n",
      "No predictions for class 191/200\n",
      "Matching predictions to ground truth, class 192/200.: 100%|███████████████████| 14560/14560 [00:00<00:00, 81800.72it/s]\n",
      "Matching predictions to ground truth, class 193/200.: 100%|███████████████████| 30002/30002 [00:00<00:00, 79583.45it/s]\n",
      "No predictions for class 194/200\n",
      "No predictions for class 195/200\n",
      "No predictions for class 196/200\n",
      "No predictions for class 197/200\n",
      "No predictions for class 198/200\n",
      "No predictions for class 199/200\n",
      "No predictions for class 200/200\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(model=model,\n",
    "                      n_classes=n_classes,\n",
    "                      data_generator=val_dataset,\n",
    "                      model_mode=model_mode)\n",
    "\n",
    "results = evaluator(img_height=img_height,\n",
    "                    img_width=img_width,\n",
    "                    batch_size=32,\n",
    "                    data_generator_mode='resize',\n",
    "                    round_confidences=False,\n",
    "                    matching_iou_threshold=0.5,\n",
    "                    border_pixels='include',\n",
    "                    sorting_algorithm='quicksort',\n",
    "                    average_precision_mode='sample',\n",
    "                    num_recall_points=11,\n",
    "                    ignore_neutral_boxes=True,\n",
    "                    return_precisions=False,\n",
    "                    return_recalls=False,\n",
    "                    return_average_precisions=False,\n",
    "                    verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
